# User Request

### Task:
Suggest 3-5 relevant follow-up questions or prompts that the user might naturally ask next in this conversation as a **user**, based on the chat history, to help continue or deepen the discussion.
### Guidelines:
- Write all follow-up questions from the user‚Äôs point of view, directed to the assistant.
- Make questions concise, clear, and directly related to the discussed topic(s).
- Only suggest follow-ups that make sense given the chat content and do not repeat what was already covered.
- If the conversation is very short or not specific, suggest more general (but relevant) follow-ups the user might ask.
- Use the conversation's primary language; default to English if multilingual.
- Response must be a JSON array of strings, no extra text or formatting.
### Output:
JSON format: { "follow_ups": ["Question 1?", "Question 2?", "Question 3?"] }
### Chat History:
<chat_history>
USER: Ich denke es schadet nicht die immer aktiv zu haben . Bitte erledige das
ASSISTANT: [Starte Claude Code...]



---

Erledigt! LSP ist jetzt permanent aktiviert.

**Was ich gemacht habe:**
- `export ENABLE_LSP_TOOL=1` zur `~/.bashrc` hinzugef√ºgt (Zeile 125)

**Wirksamkeit:**

| Wann | Aktiv? |
|------|--------|
| Neue Terminals | ‚úÖ Automatisch |
| Neue Claude Code Sessions | ‚úÖ Automatisch |
| Aktuelle Session | ‚ùå Muss neu gestartet werden |

**F√ºr sofortige Nutzung** in der aktuellen Session:
```bash
source ~/.bashrc
```
...oder Claude Code neu starten.

**Verf√ºgbare LSP-Operationen:**
- `goToDefinition` - Zur Definition springen
- `findReferences` - Alle Verwendungen finden
- `hover` - Typen/Doku anzeigen
- `documentSymbol` - Symbole in Datei
- `workspaceSymbol` - Projektweite Suche

<!-- STEP: done -->
USER: Kannst du √ºber source das Feature aktivieren und mit seiner Hilfe durch den consultant Code deep Diven um h√§sslichen Code, toten Code , nicht oder √ºberdokumentierte Sachen finden, und auch einen Vorschlag wie man mit hooks oder Tools das was das llm machen sollte wie Doku , Status Updates etc forcen oder automatisieren kann um das verhalten innerhalb von Helix deterministisch zu forcen das ganze gerne erstmal mit mir diskutieren dann √ºberlegen wir ob wir ein adr draus machen. Auch die offene bugs Liste mit ansehen. Da sind auch bugs implementiert die noch nicht als implementiert gekennzeichnet sind
ASSISTANT: [Starte Claude Code...]



---

Jetzt habe ich ein vollst√§ndiges Bild. Lass mich dir meine Analyse pr√§sentieren:

---

# Deep Dive Analyse: HELIX v4 Consultant Code

## 1. Code-Qualit√§tsprobleme

### Toter / Ungenutzter Code

| Code | Datei | Problem |
|------|-------|---------|
| `_generate_session_id_stable()` | session_manager.py:122 | **Nie aufgerufen** - nur `_generate_session_id()` wird genutzt |
| `generate_session_id()` | session_manager.py:146 | **Deprecated** - nur in `_find_or_create_session_id()` genutzt, das selbst fragw√ºrdig ist |
| `_find_or_create_session_id()` | session_manager.py:226 | Sucht via md5-Hash - aber ADR-035 hat auf uuid4 umgestellt |
| `select_experts()` | expert_manager.py:288 | **Deprecated alias** f√ºr `suggest_experts()` |
| `.bak` Dateien | api/ | 2 Backup-Dateien sollten gel√∂scht werden |

### H√§sslicher Code

1. **30+ hardcoded Pfade** `/home/aiuser01/...` verteilt √ºber:
   - `claude_runner.py` (5x)
   - `expert_manager.py` (2x)
   - `openai.py` (4x)
   - `evolution/*.py` (6x)
   - `template_engine.py`, `llm_client.py`, `context_manager.py`...
   
   **Empfehlung:** Eine `HELIX_ROOT` Environment Variable oder Config nutzen

2. **3 verschiedene Session-ID Generatoren:**
   ```python
   _generate_session_id()      # uuid4, aktiv
   _generate_session_id_stable()  # hash-based, ungenutzt
   generate_session_id()       # deprecated, noch in legacy code
   ```

3. **Step-Marker wird beim Streaming mitgestreamt** (openai.py:324-328)
   - `<!-- STEP: done -->` erscheint im User-Output
   - Sollte vorher extrahiert werden

### √úberdokumentiert

- `session.md.j2` hat **453 Zeilen** Prompt - davon ~200 Zeilen ADR-Templates die der LLM bei JEDEM Request lesen muss
- Die Doku k√∂nnte in separate Skill-Dateien ausgelagert werden

### Unterdokumentiert

- `quality_gates.py` - Die if-else Kette der Gate-Typen hat keine √úbersicht
- `SessionState.status` und `step` sind Strings ohne Enum-Definition

---

## 2. Bug-Liste Status Update

| Bug | Status in INDEX | Tats√§chlicher Status |
|-----|-----------------|---------------------|
| Bug-006 (Chat History) | üîß FIXING | **‚úÖ IMPLEMENTED** (Commit c93c1ad) |
| ADR-034 State-Machine | ‚úÖ DONE | ‚úÖ Korrekt |
| ADR-035 Rate Limiting | (nicht in bugs) | **‚úÖ IMPLEMENTED** (Commit 786ab8d) |

**Bug-006 muss auf DONE gesetzt werden!**

---

## 3. Hooks/Tools f√ºr LLM-Verhalten-Enforcement

Jetzt zur Diskussion, die du angefragt hast:

### Das Problem

Das LLM **soll** bestimmte Dinge tun:
1. **STEP-Marker setzen** (`<!-- STEP: what -->`) 
2. **Status-Updates** in status.json schreiben
3. **Dokumentation** aktuell halten
4. **ADR-Format** einhalten

Aber das LLM kann diese vergessen - und aktuell gibt es **keinen Enforcement-Mechanismus**.

### L√∂sungsans√§tze

#### Option A: Post-Response Validation Hook

```python
# Nach jeder Claude-Response
def validate_response(response_text: str) -> ValidationResult:
    issues = []
    
    # 1. STEP-Marker pr√ºfen
    if not re.search(r'<!--\s*STEP:\s*\w+\s*-->', response_text):
        issues.append("Missing STEP marker")
    
    # 2. Bei generate-Phase: ADR-Struktur pr√ºfen
    if "ADR-" in response_text:
        if "## Kontext" not in response_text:
            issues.append("ADR missing ## Kontext")
    
    return ValidationResult(valid=len(issues)==0, issues=issues)
```

**Pros:** Einfach, post-hoc korrigierbar
**Cons:** Reagiert erst nach Fehler

#### Option B: Claude Code Hooks (existiert bereits!)

Claude Code hat ein Hook-System. Wir k√∂nnten definieren:

```yaml
# config/claude-hooks.yaml
hooks:
  post_message:
    - name: step-marker-check
      command: |
        if ! grep -q '<!-- STEP:' "$RESPONSE_FILE"; then
          echo "WARNING: Missing STEP marker" >&2
          exit 1  # Optional: Fehler werfen
        fi
```

**Pros:** Native Integration
**Cons:** Nur Warn-Mechanismus, kein Retry

#### Option C: Template-Injection + Strukturierte Outputs

```jinja2
{# session.md.j2 #}
## PFLICHT-OUTPUT-FORMAT

Deine Antwort MUSS diese Struktur haben:

<response>
{{ dein_inhalt }}
</response>

<metadata>
step: {{ step }}
confidence: {{ 0-100 }}
</metadata>
```

Dann parsen wir die strukturierte Response.

**Pros:** Deterministisch wenn LLM kooperiert
**Cons:** LLM kann Format ignorieren

#### Option D: Quality Gate f√ºr Consultant Responses

```python
# Neuer Gate-Typ: consultant_response_valid
class ConsultantResponseGate(QualityGate):
    def check(self, response: str) -> GateResult:
        checks = [
            self._has_step_marker(response),
            self._has_valid_structure(response),
            self._no_hallucinated_files(response),
        ]
        return GateResult(passed=all(checks))
```

**Pros:** Integriert in bestehendes System
**Cons:** Nur f√ºr Pipeline-Phasen, nicht f√ºr Chat

### Meine Empfehlung

**Kombination aus A + B:**

1. **Post-Response Hook** der Step-Marker pr√ºft und Warnung loggt
2. **Fallback-Logic** in `_update_step_from_response()`:
   ```python
   def extract_step_from_response(self, response_text: str) -> str:
       match = re.search(r'<!--\s*STEP:\s*(\w+)\s*-->', response_text)
       if match:
           return match.group(1).lower()
       # FALLBACK: Heuristik
       if "ADR-" in response_text and "erstellt" in response_text.lower():
           return "generate"
       if "?" in response_text[-200:]:  # Fragt etwas am Ende
           return "discussing"
       return "done"  # Default
   ```

3. **Observability-Metric** f√ºr fehlende Marker (Dashboard-Widget)

---

## 4. ADR-Entscheidung

Ich sehe zwei m√∂gliche ADRs:

### ADR-038: Deterministic LLM Response Enforcement

**Scope:**
- Step-Marker Validation Hook
- Fallback-Heuristik f√ºr fehlende Marker
- Observability Dashboard f√ºr LLM-Compliance

### ADR-039: Consultant Code Cleanup

**Scope:**
- Hardcoded Pfade ‚Üí `HELIX_ROOT` Variable
- Toter Code entfernen
- `.bak` Dateien l√∂schen
- Prompt-Optimierung (Template aufteilen)

---

**Frage an dich:** 

1. Sollen wir **ein ADR** f√ºr alles machen oder **getrennt** (Enforcement vs Cleanup)?
2. Welche der Enforcement-Optionen (A-D) gef√§llt dir am besten?
3. Soll ich zuerst die **Bug-006 Status-Aktualisierung** in der INDEX.md machen?

<!-- STEP: what -->
USER: Ein gemeinsames adr. Ich bin ein Freund von retry Mechanismen. Dann dauert es vielleicht l√§nger weil Claude Code noch mal neu starten muss wird aber enforced und nicht vergessen. Kombination aus extern retry Mechanismen und Claude Code internen Warnungen Message. 
Auch beim AdR erstellen brauchen wir das. Ist das schon f√ºr AdR drin? Zb hook beim speichern der pr√ºft ob alle sections da sind und ob es die Dateien die z. B.  ob Dateien die Modifikation brauchen auch existieren . 

Wie machen wir das am elegantesten? Hook oder sub agent oder was anderes?
ASSISTANT: [Starte Claude Code...]



---

Jetzt habe ich ein vollst√§ndiges Bild. Lass mich die Diskussion zusammenfassen und einen konkreten Vorschlag machen:

---

# ADR-038 Diskussion: Deterministic LLM Response Enforcement

## Bestandsaufnahme - Was existiert bereits?

| Komponente | Was existiert | Was fehlt |
|------------|---------------|-----------|
| **SubAgentVerifier** | ‚úÖ 3-Retry-Loop mit Feedback | Nur f√ºr Pipeline-Phasen, nicht f√ºr Consultant |
| **RetryHandler** | ‚úÖ Exponential Backoff, Error-Kategorisierung | Nicht f√ºr LLM-Output-Validierung gedacht |
| **QualityGateRunner** | ‚úÖ Syntax, Files, Tests | Kein Gate f√ºr LLM-Response-Struktur |
| **ADRValidator** | ‚úÖ Sections, Header, Akzeptanzkriterien | Pr√ºft nur **nach** Erstellung, kein Enforcement |

## Das Problem

1. **STEP-Marker**: LLM kann `<!-- STEP: xxx -->` vergessen ‚Üí Step-Tracking unvollst√§ndig
2. **ADR-Erstellung**: LLM kann Sections vergessen ‚Üí Muss manuell nachgebessert werden
3. **Datei-Referenzen**: `files.modify` listet Dateien die nicht existieren ‚Üí Bricht sp√§ter
4. **Status-Updates**: LLM vergisst status.json zu aktualisieren ‚Üí Dashboard zeigt falschen Status

## Architektur-Vorschlag

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LLM Response Flow                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   Claude Code                                                    ‚îÇ
‚îÇ       ‚îÇ                                                          ‚îÇ
‚îÇ       ‚ñº                                                          ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ
‚îÇ   ‚îÇ ResponseValidator ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ HOOK: Post-Message                   ‚îÇ
‚îÇ   ‚îÇ                   ‚îÇ                                          ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ STEP-Marker     ‚îÇ                                          ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ ADR-Struktur    ‚îÇ                                          ‚îÇ
‚îÇ   ‚îÇ ‚Ä¢ File-Referenzen ‚îÇ                                          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îÇ             ‚îÇ                                                    ‚îÇ
‚îÇ             ‚ñº                                                    ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ   ‚îÇ  Validation OK?   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Weiter zu User     ‚îÇ               ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ             ‚îÇ NEIN                                               ‚îÇ
‚îÇ             ‚ñº                                                    ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ
‚îÇ   ‚îÇ  Retry-Controller ‚îÇ                                          ‚îÇ
‚îÇ   ‚îÇ  max_retries: 2   ‚îÇ                                          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îÇ             ‚îÇ                                                    ‚îÇ
‚îÇ             ‚ñº                                                    ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ
‚îÇ   ‚îÇ Claude Code       ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ Feedback-Prompt:                     ‚îÇ
‚îÇ   ‚îÇ (Continuation)    ‚îÇ     "Deine Antwort fehlt STEP-Marker..." ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Konkrete Implementierung

### Option A: Post-Response Hook (extern)

```python
# hooks/response_validator.py
class ResponseValidator:
    """Validiert LLM-Response und erzwingt Retry bei Fehlern."""
    
    def __init__(self, max_retries: int = 2):
        self.max_retries = max_retries
        self.validators = [
            StepMarkerValidator(),
            ADRStructureValidator(),
            FileExistenceValidator(),
        ]
    
    async def validate_and_retry(
        self,
        response: str,
        context: ResponseContext,
    ) -> ValidationResult:
        """
        Validiert Response. Bei Fehler:
        1. Feedback-Prompt generieren
        2. Claude Code mit --continue starten
        3. Bis max_retries wiederholen
        """
        for attempt in range(self.max_retries + 1):
            issues = self._validate(response, context)
            
            if not issues:
                return ValidationResult(success=True)
            
            if attempt < self.max_retries:
                # Retry mit Feedback
                feedback = self._generate_feedback(issues)
                response = await self._continue_with_feedback(feedback)
            else:
                # Max retries erreicht - Warnung + Fallback
                return ValidationResult(
                    success=False,
                    issues=issues,
                    fallback_applied=True
                )
```

### Option B: Claude Code Hooks Integration

Claude Code hat bereits ein Hook-System (`~/.claude/hooks/`). Wir k√∂nnten:

```yaml
# ~/.claude/hooks/post_message.yaml
hooks:
  - name: helix-response-validator
    command: |
      python -m helix.hooks.validate_response "$RESPONSE_FILE"
    on_failure: retry  # oder: warn, block
    max_retries: 2
```

**Problem**: Claude Code Hooks sind f√ºr User-facing Warnings gedacht, nicht f√ºr automatische Retries.

### Option C: Wrapper um ClaudeRunner (mein Favorit)

```python
# src/helix/enforcement/response_enforcer.py
class ResponseEnforcer:
    """Wrapper um ClaudeRunner der LLM-Output erzwingt."""
    
    def __init__(self, runner: ClaudeRunner, max_retries: int = 2):
        self.runner = runner
        self.max_retries = max_retries
        self.validators: list[ResponseValidator] = []
    
    async def run_with_enforcement(
        self,
        prompt: str,
        validators: list[str] = ["step_marker", "adr_structure"],
        **kwargs
    ) -> EnforcedResult:
        """
        F√ºhrt Claude aus und validiert Response.
        Bei Validation-Fehler: Automatic Retry mit Feedback.
        """
        active_validators = self._get_validators(validators)
        
        for attempt in range(self.max_retries + 1):
            result = await self.runner.run_phase(**kwargs)
            
            issues = []
            for validator in active_validators:
                issues.extend(validator.validate(result.stdout))
            
            if not issues:
                return EnforcedResult(
                    success=True,
                    response=result.stdout,
                    attempts=attempt + 1
                )
            
            if attempt < self.max_retries:
                # Retry mit --continue und Feedback
                feedback_prompt = self._build_feedback_prompt(issues)
                kwargs["prompt"] = feedback_prompt
                kwargs["continue_session"] = True
        
        # Fallback anwenden
        return EnforcedResult(
            success=False,
            response=self._apply_fallback(result.stdout, issues),
            issues=issues,
            fallback_applied=True
        )
```

### Validator-Module

```python
# src/helix/enforcement/validators/step_marker.py
class StepMarkerValidator(ResponseValidator):
    """Validiert dass STEP-Marker vorhanden ist."""
    
    PATTERN = r'<!--\s*STEP:\s*(\w+)\s*-->'
    VALID_STEPS = {"what", "why", "constraints", "generate", "finalize", "done"}
    
    def validate(self, response: str) -> list[ValidationIssue]:
        match = re.search(self.PATTERN, response)
        
        if not match:
            return [ValidationIssue(
                code="MISSING_STEP_MARKER",
                message="Response enth√§lt keinen STEP-Marker",
                fix_hint="F√ºge am Ende hinzu: <!-- STEP: what|why|constraints|generate|finalize|done -->"
            )]
        
        step = match.group(1).lower()
        if step not in self.VALID_STEPS:
            return [ValidationIssue(
                code="INVALID_STEP",
                message=f"Ung√ºltiger Step: {step}",
                fix_hint=f"G√ºltige Steps: {', '.join(self.VALID_STEPS)}"
            )]
        
        return []
    
    def apply_fallback(self, response: str, context: dict) -> str:
        """Fallback: Heuristik-basierte Step-Zuweisung."""
        if "ADR-" in response and "erstellt" in response.lower():
            step = "generate"
        elif "?" in response[-200:]:
            step = "discussing"  # Heuristik: Fragt etwas
        else:
            step = "done"
        
        return f"{response}\n\n<!-- STEP: {step} -->"
```

```python
# src/helix/enforcement/validators/adr_structure.py
class ADRStructureValidator(ResponseValidator):
    """Validiert ADR-Struktur wenn Response ein ADR enth√§lt."""
    
    REQUIRED_SECTIONS = ["Kontext", "Entscheidung", "Akzeptanzkriterien"]
    
    def validate(self, response: str) -> list[ValidationIssue]:
        # Nur validieren wenn es ein ADR ist
        if "---\nadr_id:" not in response:
            return []
        
        issues = []
        for section in self.REQUIRED_SECTIONS:
            if f"## {section}" not in response:
                issues.append(ValidationIssue(
                    code="MISSING_ADR_SECTION",
                    message=f"ADR fehlt Section: ## {section}",
                    fix_hint=f"F√ºge Section '## {section}' mit Inhalt hinzu"
                ))
        
        return issues
```

```python
# src/helix/enforcement/validators/file_existence.py
class FileExistenceValidator(ResponseValidator):
    """Pr√ºft ob referenzierte Dateien existieren."""
    
    def validate(self, response: str) -> list[ValidationIssue]:
        # Extrahiere files.modify aus YAML
        yaml_match = re.search(r'---\n(.*?)\n---', response, re.DOTALL)
        if not yaml_match:
            return []
        
        try:
            metadata = yaml.safe_load(yaml_match.group(1))
            modify_files = metadata.get("files", {}).get("modify", [])
        except:
            return []
        
        issues = []
        for file_path in modify_files:
            full_path = Path(HELIX_ROOT) / file_path
            if not full_path.exists():
                issues.append(ValidationIssue(
                    code="FILE_NOT_FOUND",
                    message=f"files.modify referenziert nicht existierende Datei: {file_path}",
                    fix_hint=f"Entferne '{file_path}' aus files.modify oder verschiebe nach files.create"
                ))
        
        return issues
```

## Bug-006 Status Update

Bug-006 (Chat-History) ist bereits in Commit `c93c1ad` gefixt. Die INDEX.md muss aktualisiert werden:

```diff
- **Status:** üîß FIXING
+ **Status:** ‚úÖ DONE (Commit c93c1ad, 2025-12-30)
```

---

## Meine Empfehlung

**Option C (Wrapper um ClaudeRunner)** weil:

1. **Zentrale Integration**: Alle Claude-Aufrufe gehen durch einen Punkt
2. **Retry mit --continue**: Nutzt Claude's Session-Continuation
3. **Fallback-Mechanismus**: Wenn max_retries erreicht, wird Heuristik angewandt
4. **Erweiterbar**: Neue Validators k√∂nnen einfach hinzugef√ºgt werden
5. **Logging/Observability**: Zentral f√ºr alle Enforcement-Aktionen

---

## Fragen an dich

1. **Retry-Strategie**: 
   - Soll Claude Code komplett neu gestartet werden (langsam, aber sauberer State)?
   - Oder mit `--continue` weitermachen (schnell, aber Kontext-Wachstum)?

2. **Fallback-Verhalten**:
   - Soll bei max_retries eine Warnung geloggt werden und trotzdem weitermachen?
   - Oder soll die Operation abbrechen?

3. **Validator-Scope**:
   - Nur f√ºr Consultant-Responses?
   - Oder f√ºr alle Claude-Aufrufe (Pipeline-Phasen)?

4. **Soll ich jetzt das ADR schreiben** mit Option C als Empfehlung, oder willst du noch etwas diskutieren?

<!-- STEP: what -->
</chat_history>

---
adr_id: "030"
title: "Evolution Pipeline Reliability"
status: Proposed

project_type: helix_internal
component_type: PROCESS
classification: FIX
change_scope: minor

files:
  create:
    - src/helix/pipeline/retry_handler.py
    - tests/pipeline/test_retry_handler.py
  modify:
    - src/helix/claude_runner.py
    - src/helix/api/streaming.py
    - src/helix/api/routes.py
    - src/helix/phase_status.py
  docs:
    - docs/ARCHITECTURE-MODULES.md

depends_on:
  - ADR-011  # Post-Phase Verification
  - ADR-025  # Sub-Agent Verifikation
---

# ADR-030: Evolution Pipeline Reliability

## Status

üìã Proposed

---

## Kontext

### Das Problem

Bei der Ausf√ºhrung von Evolution Pipelines wurden 4 systematische Issues identifiziert, die die Zuverl√§ssigkeit und Beobachtbarkeit beeintr√§chtigen:

### Issue 1: pytest PATH Issue (Exit 127)

**Symptom:** Claude Code Instanzen k√∂nnen `pytest` nicht finden.

```bash
$ pytest tests/
/bin/bash: pytest: command not found
Exit code: 127
```

**Ursache:** Der Claude Code Runner startet Bash ohne aktivierte Python-Umgebung. Das helix virtualenv wird nicht geladen.

**Auswirkung:** Phasen mit `quality_gate: tests_pass` schlagen fehl, obwohl Tests korrekt w√§ren.

### Issue 2: Job Status API zeigt keine Phasen

**Symptom:** `GET /helix/jobs/{id}` gibt `phases: []` zur√ºck, obwohl Phasen laufen.

```json
{
  "id": "job-123",
  "status": "running",
  "phases": []  // <- Sollte Phasen enthalten
}
```

**Ursache:** Die Phasen-Information wird w√§hrend der Ausf√ºhrung nicht in `JobState` geschrieben. Nur `status.json` wird aktualisiert, aber die API liest aus dem In-Memory `JobState`.

**Auswirkung:** Keine Sichtbarkeit √ºber den Fortschritt via API. User m√ºssen manuell `status.json` lesen.

### Issue 3: StrEnum Test-Kompatibilit√§t

**Symptom:** Tests mit `StrEnum` schlagen auf Python 3.10 fehl.

```python
# src/helix/phase_status.py
class PhaseStatus(StrEnum):  # Python 3.11+
    PENDING = "pending"
```

**Ursache:** `StrEnum` wurde erst in Python 3.11 in die stdlib aufgenommen. HELIX deklariert Python 3.10+ Kompatibilit√§t.

**Auswirkung:** CI/CD Pipelines auf Python 3.10 Systemen brechen.

### Issue 4: Keine automatische Retry bei kleinen Fehlern

**Symptom:** Transiente Fehler (Netzwerk, API Throttling) f√ºhren zu sofortigem Phase-Fail.

```
Phase 2: API Error 429 Too Many Requests
Status: FAILED
```

**Ursache:** Es gibt keinen Retry-Mechanismus f√ºr transiente Fehler. Jeder Fehler f√ºhrt sofort zum Abbruch.

**Auswirkung:** Unn√∂tige manuelle Neustarts bei tempor√§ren Problemen.

---

## Entscheidung

### Wir entscheiden uns f√ºr:

Eine koordinierte Fix-Sammlung die alle 4 Issues addressiert.

### Diese Entscheidung beinhaltet:

1. **PATH Fix:** Shell-Environment Setup im Claude Runner
2. **API Sync:** Phasen-Status in JobState synchronisieren
3. **StrEnum Backport:** Kompatibilit√§t f√ºr Python 3.10
4. **Retry Handler:** Exponential Backoff f√ºr transiente Fehler

### Warum diese L√∂sung?

- Alle 4 Issues sind unabh√§ngig voneinander
- Jeder Fix ist isoliert und testbar
- Kein Breaking Change an bestehenden APIs
- Backward-kompatibel

---

## Implementation

### Fix 1: pytest PATH Issue

**Datei:** `src/helix/claude_runner.py`

Das Shell-Environment muss den helix virtualenv aktivieren bevor Befehle ausgef√ºhrt werden:

```python
# src/helix/claude_runner.py

class ClaudeRunner:
    """Claude Code CLI wrapper with environment setup."""

    def __init__(self, venv_path: Path | None = None):
        self.venv_path = venv_path or Path("/home/aiuser01/helix-v4/.venv")

    def _get_shell_env(self) -> dict[str, str]:
        """Get shell environment with virtualenv activated."""
        env = os.environ.copy()

        if self.venv_path.exists():
            venv_bin = self.venv_path / "bin"
            env["PATH"] = f"{venv_bin}:{env.get('PATH', '')}"
            env["VIRTUAL_ENV"] = str(self.venv_path)

        return env

    async def run_phase_streaming(
        self,
        project_path: Path,
        phase: PhaseConfig,
        **kwargs
    ) -> PhaseResult:
        """Run a phase with proper environment setup."""
        env = self._get_shell_env()

        # Verify pytest is accessible
        pytest_check = subprocess.run(
            ["which", "pytest"],
            env=env,
            capture_output=True
        )
        if pytest_check.returncode != 0:
            raise EnvironmentError(
                f"pytest not found in PATH. "
                f"Ensure virtualenv at {self.venv_path} is valid."
            )

        # ... rest of implementation
```

### Fix 2: Job Status API Phasen-Synchronisation

**Datei:** `src/helix/api/streaming.py`

Die Phase-Updates m√ºssen in den JobState geschrieben werden:

```python
# src/helix/api/streaming.py

async def stream_phase_execution(
    job_id: str,
    job_state: JobState,
    phases: list[PhaseConfig],
    runner: ClaudeRunner
) -> AsyncIterator[StreamEvent]:
    """Execute phases with JobState synchronization."""

    for phase in phases:
        # Update JobState BEFORE starting phase
        phase_info = PhaseInfo(
            id=phase.id,
            name=phase.name,
            status=PhaseStatus.RUNNING,
            started_at=datetime.now()
        )
        job_state.add_phase(phase_info)

        yield StreamEvent(type="phase_start", data=phase_info.dict())

        try:
            result = await runner.run_phase_streaming(...)

            # Update JobState AFTER phase completion
            phase_info.status = PhaseStatus.COMPLETED if result.success else PhaseStatus.FAILED
            phase_info.completed_at = datetime.now()
            job_state.update_phase(phase_info)

            yield StreamEvent(type="phase_complete", data=phase_info.dict())

        except Exception as e:
            phase_info.status = PhaseStatus.FAILED
            phase_info.error = str(e)
            job_state.update_phase(phase_info)
            raise
```

**Datei:** `src/helix/api/routes.py`

Der Endpoint muss die Phasen aus JobState lesen:

```python
# src/helix/api/routes.py

@router.get("/jobs/{job_id}")
async def get_job_status(job_id: str) -> JobStatusResponse:
    """Get job status including phase information."""
    job_state = job_manager.get_job(job_id)

    if not job_state:
        raise HTTPException(status_code=404, detail="Job not found")

    return JobStatusResponse(
        id=job_id,
        status=job_state.status,
        phases=[p.dict() for p in job_state.phases],  # <- Phasen inkludieren
        started_at=job_state.started_at,
        completed_at=job_state.completed_at,
        error=job_state.error
    )
```

### Fix 3: StrEnum Python 3.10 Kompatibilit√§t

**Datei:** `src/helix/phase_status.py`

Fallback f√ºr Python < 3.11:

```python
# src/helix/phase_status.py

"""Phase status enumeration with Python 3.10 compatibility."""

import sys
from enum import Enum

if sys.version_info >= (3, 11):
    from enum import StrEnum
else:
    # Backport for Python 3.10
    class StrEnum(str, Enum):
        """String enumeration compatible with Python 3.10+."""

        def __str__(self) -> str:
            return self.value

        def __repr__(self) -> str:
            return f"{self.__class__.__name__}.{self.name}"


class PhaseStatus(StrEnum):
    """Status of a pipeline phase."""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"
```

### Fix 4: Automatische Retry bei transienten Fehlern

**Datei:** `src/helix/pipeline/retry_handler.py`

Neues Modul f√ºr Retry-Logik:

```python
# src/helix/pipeline/retry_handler.py

"""Retry handler for transient pipeline errors."""

import asyncio
from dataclasses import dataclass
from enum import Enum
from typing import TypeVar, Callable, Awaitable
import logging

logger = logging.getLogger(__name__)

T = TypeVar("T")


class ErrorCategory(Enum):
    """Categorization of errors for retry decisions."""

    TRANSIENT = "transient"      # Network, throttling - retry
    PERMANENT = "permanent"       # Syntax, logic - no retry
    UNKNOWN = "unknown"          # Default - retry once


# Patterns that indicate transient errors
TRANSIENT_PATTERNS = [
    "429",
    "Too Many Requests",
    "rate limit",
    "timeout",
    "connection reset",
    "temporary failure",
    "ETIMEDOUT",
    "ECONNRESET",
]


@dataclass
class RetryConfig:
    """Configuration for retry behavior."""

    max_retries: int = 3
    initial_delay: float = 1.0
    max_delay: float = 30.0
    exponential_base: float = 2.0


def categorize_error(error: Exception) -> ErrorCategory:
    """Categorize an error to determine retry strategy."""
    error_msg = str(error).lower()

    for pattern in TRANSIENT_PATTERNS:
        if pattern.lower() in error_msg:
            return ErrorCategory.TRANSIENT

    # Permanent errors - no retry
    if any(p in error_msg for p in ["syntax", "import", "name error"]):
        return ErrorCategory.PERMANENT

    return ErrorCategory.UNKNOWN


async def with_retry(
    func: Callable[[], Awaitable[T]],
    config: RetryConfig | None = None,
    error_context: str = "operation"
) -> T:
    """Execute an async function with retry on transient errors.

    Args:
        func: Async function to execute
        config: Retry configuration (uses defaults if None)
        error_context: Context string for error messages

    Returns:
        Result of the function

    Raises:
        Last exception if all retries exhausted
    """
    config = config or RetryConfig()
    last_error: Exception | None = None

    for attempt in range(config.max_retries + 1):
        try:
            return await func()
        except Exception as e:
            last_error = e
            category = categorize_error(e)

            if category == ErrorCategory.PERMANENT:
                logger.warning(
                    f"{error_context}: Permanent error, no retry. {e}"
                )
                raise

            if attempt >= config.max_retries:
                logger.error(
                    f"{error_context}: Max retries ({config.max_retries}) "
                    f"exhausted. Last error: {e}"
                )
                raise

            delay = min(
                config.initial_delay * (config.exponential_base ** attempt),
                config.max_delay
            )

            logger.info(
                f"{error_context}: Attempt {attempt + 1} failed ({category.value}), "
                f"retrying in {delay:.1f}s. Error: {e}"
            )

            await asyncio.sleep(delay)

    # Should not reach here, but satisfy type checker
    raise last_error  # type: ignore
```

**Integration in streaming.py:**

```python
# src/helix/api/streaming.py

from helix.pipeline.retry_handler import with_retry, RetryConfig

async def stream_phase_execution(...):
    for phase in phases:
        # ... phase setup ...

        result = await with_retry(
            lambda: runner.run_phase_streaming(project_path, phase),
            config=RetryConfig(max_retries=2),
            error_context=f"Phase {phase.id}: {phase.name}"
        )
```

### Fix 5: Baseline-basierte Test-Bewertung

**Problem:** Die Pipeline stoppt bei jedem einzelnen Test-Failure, auch wenn:
- Der Test schon vorher kaputt war (pre-existing failure)
- Der Test nicht zur aktuellen √Ñnderung geh√∂rt
- Es nur 1 von 500 Tests ist (0.2% Failure Rate)

**Symptom (ADR-020):**
```
510 passed, 1 failed ‚Üí Pipeline STOPPED
```
Der eine Failure war ein StrEnum-Test der schon vorher nicht funktioniert h√§tte.

**L√∂sung:** Automatische Baseline-Erkennung mit Diff-basierter Bewertung.

#### Konzept
```
VOR Pipeline:    pytest main branch ‚Üí Baseline (z.B. 3 failures)
NACH Pipeline:   pytest mit √Ñnderungen ‚Üí Aktuell (z.B. 4 failures)
                         ‚îÇ
                         ‚ñº
                 DIFF-ANALYSE
                 ‚îú‚îÄ 3 failures waren schon in Baseline ‚Üí IGNORIEREN
                 ‚îî‚îÄ 1 failure ist NEU ‚Üí BLOCKING (Regression)
```

#### Kategorien

| Kategorie | Bedeutung | Aktion |
|-----------|-----------|--------|
| **Pre-existing** | Failure war schon in Baseline | ‚úÖ Ignorieren |
| **Regression** | Bestehender Test jetzt kaputt | ‚ùå Blocking |
| **New Test Failure** | Neuer Test (aus ADR) failt | ‚ùå Blocking |

#### Implementation

**Datei:** `src/helix/evolution/test_baseline.py`
```python
"""Baseline-based test evaluation for Evolution Pipeline."""

from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import subprocess
import json


@dataclass
class TestBaseline:
    """Snapshot of test results before changes."""
    
    timestamp: datetime
    commit_sha: str
    total_tests: int
    passed_tests: int
    failed_tests: set[str] = field(default_factory=set)  # nodeid format
    
    def to_dict(self) -> dict:
        return {
            "timestamp": self.timestamp.isoformat(),
            "commit_sha": self.commit_sha,
            "total_tests": self.total_tests,
            "passed_tests": self.passed_tests,
            "failed_tests": list(self.failed_tests)
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> "TestBaseline":
        return cls(
            timestamp=datetime.fromisoformat(data["timestamp"]),
            commit_sha=data["commit_sha"],
            total_tests=data["total_tests"],
            passed_tests=data["passed_tests"],
            failed_tests=set(data["failed_tests"])
        )


@dataclass
class TestEvaluationResult:
    """Result of baseline comparison."""
    
    passed: bool
    total_tests: int
    passed_tests: int
    
    # Categorized failures
    pre_existing: list[str]      # Already failed in baseline (OK)
    regressions: list[str]       # Newly broken existing tests (BLOCKING)
    new_test_failures: list[str] # New tests that fail (BLOCKING)
    
    # Summary
    blocking_failures: list[str]
    ignored_failures: list[str]
    
    @property
    def summary(self) -> str:
        if self.passed:
            ignored = len(self.ignored_failures)
            return (
                f"‚úÖ Tests passed: {self.passed_tests}/{self.total_tests}"
                f"{f' ({ignored} pre-existing failures ignored)' if ignored else ''}"
            )
        else:
            return (
                f"‚ùå Tests failed: {len(self.blocking_failures)} blocking failures\n"
                f"   Regressions: {self.regressions}\n"
                f"   New test failures: {self.new_test_failures}"
            )


async def capture_baseline(project_root: Path) -> TestBaseline:
    """
    Capture test baseline from current state (before changes).
    
    Called at the START of pipeline execution, before any phases run.
    Uses pytest JSON output for reliable parsing.
    """
    result = subprocess.run(
        ["python3", "-m", "pytest", "--json-report", "--json-report-file=/dev/stdout", "-q"],
        cwd=project_root,
        capture_output=True,
        text=True,
        env={**os.environ, "PYTHONPATH": f"{project_root}/src"}
    )
    
    try:
        report = json.loads(result.stdout)
    except json.JSONDecodeError:
        # Fallback: parse pytest output manually
        return _parse_pytest_text_output(result.stdout, result.stderr)
    
    failed = {
        test["nodeid"]
        for test in report.get("tests", [])
        if test["outcome"] == "failed"
    }
    
    summary = report.get("summary", {})
    
    return TestBaseline(
        timestamp=datetime.now(),
        commit_sha=_get_current_commit(project_root),
        total_tests=summary.get("total", 0),
        passed_tests=summary.get("passed", 0),
        failed_tests=failed
    )


def evaluate_against_baseline(
    current_failures: set[str],
    current_total: int,
    current_passed: int,
    baseline: TestBaseline,
    adr_test_files: list[str]
) -> TestEvaluationResult:
    """
    Compare current test results against baseline.
    
    Args:
        current_failures: Set of failed test nodeids from current run
        current_total: Total tests in current run
        current_passed: Passed tests in current run
        baseline: Baseline captured before changes
        adr_test_files: Test files created by this ADR (from files.create)
    
    Returns:
        TestEvaluationResult with categorized failures
    """
    # Pre-existing: failures that were already in baseline
    pre_existing = current_failures & baseline.failed_tests
    
    # New failures: not in baseline
    new_failures = current_failures - baseline.failed_tests
    
    # Split new failures: from ADR tests vs regressions in existing tests
    new_test_failures = []
    regressions = []
    
    for failure in new_failures:
        # Check if this test file was created by the ADR
        test_file = failure.split("::")[0]
        if any(test_file.endswith(adr_file.lstrip("./")) for adr_file in adr_test_files):
            new_test_failures.append(failure)
        else:
            regressions.append(failure)
    
    # Blocking = regressions + new test failures
    blocking = regressions + new_test_failures
    
    return TestEvaluationResult(
        passed=len(blocking) == 0,
        total_tests=current_total,
        passed_tests=current_passed,
        pre_existing=list(pre_existing),
        regressions=regressions,
        new_test_failures=new_test_failures,
        blocking_failures=blocking,
        ignored_failures=list(pre_existing)
    )


def _get_current_commit(project_root: Path) -> str:
    """Get current git commit SHA."""
    result = subprocess.run(
        ["git", "rev-parse", "HEAD"],
        cwd=project_root,
        capture_output=True,
        text=True
    )
    return result.stdout.strip()[:8]
```

**Integration in Pipeline:** `src/helix/evolution/validator.py`
```python
async def validate_with_baseline(
    project: EvolutionProject,
    baseline: TestBaseline
) -> ValidationResult:
    """
    Validate project with baseline comparison.
    
    Only NEW failures block integration.
    Pre-existing failures are logged but ignored.
    """
    # Get ADR test files
    adr = project.get_adr()
    adr_test_files = [
        f for f in adr.files.get("create", [])
        if f.startswith("tests/") or "/tests/" in f
    ]
    
    # Run pytest on test system
    test_result = await run_pytest_json(project.test_system_path)
    
    current_failures = {t["nodeid"] for t in test_result["tests"] if t["outcome"] == "failed"}
    
    # Evaluate against baseline
    eval_result = evaluate_against_baseline(
        current_failures=current_failures,
        current_total=test_result["summary"]["total"],
        current_passed=test_result["summary"]["passed"],
        baseline=baseline,
        adr_test_files=adr_test_files
    )
    
    # Log details
    if eval_result.ignored_failures:
        logger.info(
            f"Ignoring {len(eval_result.ignored_failures)} pre-existing failures: "
            f"{eval_result.ignored_failures[:5]}..."
        )
    
    if eval_result.passed:
        return ValidationResult(success=True, message=eval_result.summary)
    else:
        return ValidationResult(
            success=False,
            message=eval_result.summary,
            details={
                "regressions": eval_result.regressions,
                "new_test_failures": eval_result.new_test_failures,
                "pre_existing_ignored": eval_result.ignored_failures
            }
        )
```

**Pipeline-Erweiterung:** `src/helix/api/routes_evolution.py`
```python
@router.post("/evolution/projects/{name}/run")
async def run_evolution_pipeline(name: str, auto_integrate: bool = False):
    """Run evolution pipeline with baseline-based validation."""
    
    project = project_manager.get_project(name)
    
    # STEP 1: Capture baseline BEFORE any changes
    logger.info("Capturing test baseline...")
    baseline = await capture_baseline(project.production_path)
    logger.info(
        f"Baseline: {baseline.passed_tests}/{baseline.total_tests} passed, "
        f"{len(baseline.failed_tests)} pre-existing failures"
    )
    
    # STEP 2: Execute phases
    async for event in execute_phases(project):
        yield event
    
    # STEP 3: Deploy to test system
    await deploy_to_test(project)
    
    # STEP 4: Validate with baseline comparison
    validation = await validate_with_baseline(project, baseline)
    
    if validation.success:
        logger.info(validation.message)
        if auto_integrate:
            await integrate_project(project)
    else:
        logger.error(validation.message)
        # Pipeline stops but with detailed breakdown
```

#### Beispiel-Output

**Szenario:** ADR-020 mit dem StrEnum-Problem
```
=== Baseline Capture ===
Commit: 61d0428 (main)
Tests: 453 passed, 3 failed
Pre-existing failures:
  - tests/legacy/test_old_export.py::test_deprecated
  - tests/integration/test_external.py::test_flaky
  - tests/docs/test_reverse_index.py::test_file_status_string_enum

=== After ADR-020 ===
Tests: 509 passed, 4 failed

=== Baseline Comparison ===
Pre-existing (ignored): 3
  - tests/legacy/test_old_export.py::test_deprecated
  - tests/integration/test_external.py::test_flaky  
  - tests/docs/test_reverse_index.py::test_file_status_string_enum

Regressions: 0
New test failures: 1
  - tests/docs/test_skill_selector.py::test_edge_case  ‚Üê BLOCKING

‚ùå Pipeline stopped: 1 blocking failure (new test)
```

vs. wenn der StrEnum-Test der einzige Failure ist:
```
=== Baseline Comparison ===
Pre-existing (ignored): 1
  - tests/docs/test_reverse_index.py::test_file_status_string_enum

Regressions: 0
New test failures: 0

‚úÖ Tests passed: 510/511 (1 pre-existing failure ignored)
‚Üí Proceeding to integration
```

#### Optional: Permanente Ausnahmen

F√ºr Tests die **immer** ignoriert werden sollen (unabh√§ngig von Baseline):

**Datei:** `tests/.permanent_skips` (optional)
```yaml
# Tests die IMMER ignoriert werden - nur f√ºr Sonderf√§lle!
# Normalerweise reicht die automatische Baseline-Erkennung.

tests/integration/test_external_api.py::test_paid_service:
  reason: "Ben√∂tigt API-Key der nur in Prod existiert"
  
tests/e2e/test_browser.py::test_selenium:
  reason: "Kein Browser in CI-Environment"
```

Diese werden VOR der Baseline-Analyse entfernt.

---



---

## Konsequenzen Fix 5

### Vorteile

1. **Keine manuelle Pflege:** Baseline wird automatisch erfasst
2. **Fair:** Nur NEUE Probleme blockieren, nicht geerbte
3. **Transparent:** Klare Kategorisierung im Output
4. **Realistisch:** Alte Tech Debt blockiert nicht neue Features

### Nachteile

1. **Baseline-Zeit:** Extra pytest-Run am Anfang (~30s)
2. **Speicher:** Baseline muss w√§hrend Pipeline gehalten werden

### Risiken & Mitigation

| Risiko | Mitigation |
|--------|------------|
| Baseline veraltet bei langer Pipeline | Baseline gilt f√ºr diesen Run, n√§chster Run = neue Baseline |
| Flaky Tests mal in Baseline, mal nicht | Flaky Tests sollten generell gefixt werden |
| Baseline-Capture schl√§gt fehl | Fallback: Threshold-basiert (95% m√ºssen passen) |


---

## Dokumentation

| Dokument | √Ñnderung |
|----------|----------|
| `docs/ARCHITECTURE-MODULES.md` | Retry Handler Modul dokumentieren |
| Code-Docstrings | Alle neuen Funktionen dokumentieren |

---

## Akzeptanzkriterien

### Fix 1: pytest PATH

- [ ] `pytest` ist in Claude Code Bash-Sessions verf√ºgbar
- [ ] Test: Phase mit `quality_gate: tests_pass` l√§uft erfolgreich
- [ ] Fehler bei fehlendem virtualenv ist aussagekr√§ftig

### Fix 2: Job Status API

- [ ] `GET /helix/jobs/{id}` gibt `phases` Array zur√ºck
- [ ] Jede Phase hat `id`, `name`, `status`, `started_at`
- [ ] Abgeschlossene Phasen haben `completed_at`
- [ ] Test: E2E Test f√ºr Job-Status w√§hrend Ausf√ºhrung

### Fix 3: StrEnum Kompatibilit√§t

- [ ] Tests laufen auf Python 3.10
- [ ] Tests laufen auf Python 3.11+
- [ ] `PhaseStatus` Enum funktioniert identisch auf beiden Versionen

### Fix 4: Retry Handler

- [ ] Transiente Fehler (429, timeout) werden automatisch wiederholt
- [ ] Permanente Fehler (SyntaxError) werden nicht wiederholt
- [ ] Max 3 Retries mit exponential backoff
- [ ] Retry-Versuche werden geloggt
- [ ] Unit Tests f√ºr `categorize_error` und `with_retry`

## Akzeptanzkriterien Fix 5

- [ ] Baseline wird automatisch vor Pipeline-Start erfasst
- [ ] Baseline enth√§lt: commit_sha, timestamp, failed_tests Set
- [ ] Pre-existing failures werden ignoriert (nicht blocking)
- [ ] Regressions (bestehende Tests neu kaputt) sind blocking
- [ ] Neue Test-Failures (aus ADR files.create) sind blocking
- [ ] Pipeline-Output zeigt klare Kategorisierung
- [ ] Test: 510/511 passed mit 1 pre-existing ‚Üí ‚úÖ Integration
- [ ] Test: 510/511 passed mit 1 regression ‚Üí ‚ùå Blocked
- [ ] Test: Neuer Test aus ADR failt ‚Üí ‚ùå Blocked
- [ ] Optional: `.permanent_skips` f√ºr Sonderf√§lle

### Integration

- [ ] Alle 5 Fixes funktionieren zusammen
- [ ] Keine Regression in bestehenden Pipelines
- [ ] Pipeline-Ausf√ºhrung ist stabiler als zuvor

---

## Konsequenzen

### Vorteile

1. **H√∂here Zuverl√§ssigkeit:** Transiente Fehler f√ºhren nicht mehr zu sofortigem Abbruch
2. **Bessere Observability:** Phasen-Status ist via API sichtbar
3. **Breitere Kompatibilit√§t:** Python 3.10 wird unterst√ºtzt
4. **Weniger manuelle Intervention:** Automatische Retries reduzieren Wartungsaufwand

### Nachteile

1. **Komplexit√§t:** Retry-Logik muss korrekt implementiert werden
2. **Latenz:** Retries verz√∂gern die Gesamt-Ausf√ºhrung bei Fehlern

### Risiken & Mitigation

| Risiko | Mitigation |
|--------|------------|
| Endlos-Retries | Max 3 Retries fest konfiguriert |
| Falsche Error-Kategorisierung | Pattern-Liste wartbar, einfach erweiterbar |
| Race Conditions bei JobState | JobState-Updates sind thread-safe |

---

## Abh√§ngigkeiten

- **ADR-011:** Post-Phase Verification - Integriert mit Retry
- **ADR-025:** Sub-Agent Verifikation - Retry vor Verification

---

## Rollout Plan

1. **Phase 1:** StrEnum Fix (sofort, breaking risk: low)
2. **Phase 2:** Job Status API Fix (nach Tests)
3. **Phase 3:** pytest PATH Fix (nach Tests)
4. **Phase 4:** Retry Handler (nach Integration Tests)

---

## Referenzen

- [ADR-011: Post-Phase Verification](011-post-phase-verification.md)
- [ADR-025: Sub-Agent Verifikation](025-sub-agent-verification.md)
- [Python StrEnum Documentation](https://docs.python.org/3/library/enum.html#enum.StrEnum)
